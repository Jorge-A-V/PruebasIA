{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARS = [ \"a\", \"b\" ]\n",
    "tokenize = lambda s : [CHARS.index(c) for c in s]\n",
    "decode = lambda token : CHARS[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0]\n",
      "b\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(\"aabaa\"))\n",
    "print(decode(1))\n",
    "print(decode(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# [m, in] [in, out], [out] --> [m, out]\n",
    "def linear(x, w, b):\n",
    "    return x @ w + b\n",
    "\n",
    "# [ nq, dk] [nk, dk] [nk, dv] [nq, nk] --> [nq, dv]\n",
    "def attention(query, key, value, mask):\n",
    "    return softmax(query @ key.T / np.sqrt(query.shape[-1]) + mask) @ value\n",
    "\n",
    "# [n_seq, n_embd ] --> [ n_seq, n_embd ]\n",
    "def casual_self_attention(x, c_attn, c_proj):\n",
    "\n",
    "    #qkv projections\n",
    "    x = linear(x, **c_attn) # [ n_seq, n_embd ] --> [ n_seq, 3*n_embd ]\n",
    "\n",
    "    # [ n_seq, 3* n_emdd ] --> 3 * [n _seq, n_embd]\n",
    "    q, k, v = np.split(x, 3, axis=-1)\n",
    "\n",
    "    #masking [n_seq, n_seq]\n",
    "    casual_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10\n",
    "\n",
    "    #casual attention [n_seq, n_seq ] --> [n_seq, n_seq]\n",
    "    x = attention(q,k,v,casual_mask)\n",
    "\n",
    "    #out [n_seq, n_embd] @ [n_embd, n_embd] --> [n_seq, n_embd]\n",
    "    x = linear(x, **c_proj)\n",
    "\n",
    "    return x\n",
    "\n",
    "def transformer_block(x, attn):\n",
    "    x = x + casual_self_attention(x, **attn)\n",
    "    return x\n",
    "\n",
    "# [n_seq] --> [n_seq, n_vocab]\n",
    "def gpt(inputs, wte, wpe, blocks):\n",
    "    #token && positional embeddings\n",
    "    # [n_seq] --> [n_seq, n_embd]\n",
    "    x = wte[inputs] + wpe[range(len(inputs))]\n",
    "\n",
    "    #forward layers\n",
    "    for block in blocks:\n",
    "        # [n_seq, n_embd] --> [n_seq, n_embd]\n",
    "        x = transformer_block(x, **block) \n",
    "\n",
    "    #project to vocab\n",
    "    return x @ wte.T # [n_seq, n_embd] --> [n_seq, n_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_legnth = 5\n",
    "vocab = len(CHARS)\n",
    "n_embd = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lg = 1024\n",
    "MODEL = {\n",
    "    \"wte\": np.array(\n",
    "    [\n",
    "        [0,0,0,0,0,1,0,0],\n",
    "        [0,0,0,0,0,0,1,0]\n",
    "    ]\n",
    "    ),\n",
    "    \"wpe\": np.array(\n",
    "    [\n",
    "        [1,0,0,0,0,0,0,0],\n",
    "        [0,1,0,0,0,0,0,0],\n",
    "        [0,0,1,0,0,0,0,0],\n",
    "        [0,0,0,1,0,0,0,0],\n",
    "        [0,0,0,0,1,0,0,0]\n",
    "    ]\n",
    "    ),\n",
    "    \"blocks\": [\n",
    "        {\n",
    "            \"attn\": {\n",
    "                \"c_attn\": {\n",
    "                    \"b\": np.zeros(n_embd * 3),\n",
    "                    \"w\": np.array(\n",
    "                        #fmt off\n",
    "                        [\n",
    "                            [Lg, 0., 0., 0., 0., 0., 0., 0.,  # q\n",
    "                                1., 0., 0., 0., 0., 0., 0., 0.,  # k\n",
    "                                    0., 0., 0., 0., 0., 0., 0., 0.], # v\n",
    "                            [Lg, Lg, 0., 0., 0., 0., 0., 0.,  # q\n",
    "                                0., 1., 0., 0., 0., 0., 0., 0.,  # k\n",
    "                                    0., 0., 0., 0., 0., 0., 0., 0.], # v\n",
    "                            [0., Lg, Lg, 0., 0., 0., 0., 0.,  # q\n",
    "                                0., 0., 1., 0., 0., 0., 0., 0.,  # k\n",
    "                                    0., 0., 0., 0., 0., 0., 0., 0.], # v\n",
    "                            [0., 0., Lg, Lg, 0., 0., 0., 0.,  # q\n",
    "                                0., 0., 0., 1., 0., 0., 0., 0.,  # k\n",
    "                                    0., 0., 0., 0., 0., 0., 0., 0.], # v\n",
    "                            [0., 0., 0., Lg, Lg, 0., 0., 0.,  # q\n",
    "                                0., 0., 0., 0., 1., 0., 0., 0.,  # k\n",
    "                                    0., 0., 0., 0., 0., 0., 0., 0.], # v\n",
    "                            [0., 0., 0., 0., 0., 0., 0., 0.,  # q\n",
    "                                0., 0., 0., 0., 0., 0., 0., 0.,  # k\n",
    "                                    0., 0., 0., 0., 0., 0., 0., 1.], # v\n",
    "                            [0., 0., 0., 0., 0., 0., 0., 0.,  # q\n",
    "                                0., 0., 0., 0., 0., 0., 0., 0.,  # k\n",
    "                                    0., 0., 0., 0., 0., 0., 0., -1], # v\n",
    "                            [0., 0., 0., 0., 0., 0., 0., 0.,  # q\n",
    "                                0., 0., 0., 0., 0., 0., 0., 0.,  # k\n",
    "                                    0., 0., 0., 0., 0., 0., 0., 0.], # v\n",
    "                        ]\n",
    "                        #fmt on\n",
    "                    )},\n",
    "                \"c_proj\": {  # weights to project attn result back to embedding space\n",
    "                    \"b\": [0, 0, 0, 0, 0, Lg, 0, 0],\n",
    "                    \"w\": np.array([\n",
    "                        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                        [0, 0, 0, 0, 0, -Lg, Lg, 0],\n",
    "                    ]),\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    ] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete(s, max_new_tokens = 10):\n",
    "    tokens = tokenize(s)\n",
    "    while len(tokens) < len(s) + max_new_tokens:\n",
    "        logits = gpt(np.array(tokens[-5:]), **MODEL)\n",
    "        probs = softmax(logits)\n",
    "        pred = np.argmax(probs[-1])\n",
    "        tokens.append(pred)\n",
    "    return s + \" :: \" + \"\".join(decode(t) for t in tokens[len(s):])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a :: baabaabaab\n"
     ]
    }
   ],
   "source": [
    "print(complete(\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a :: baabaabaab\n"
     ]
    }
   ],
   "source": [
    "print(complete(\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

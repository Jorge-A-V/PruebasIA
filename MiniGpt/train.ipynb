{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "djasjdalskdlaksdjasljd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tamaño : 805521\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset tamaño : {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCABULARIO: \t\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_abcdefghijklmnopqrstuvwxyz{|}~ ¡ª´¿ÉÍÑÜáçéíñóúı˜̴̵̷̸̡̢̧̨̛̖̗̘̙̜̝̞̟̠̣̤̥̦̩̪̫̬̭̮̯̰̱̲̳̹̺̻̼͇͈͉͍͎͓͔͕͖͙͚̀́̂̃̄̅̆̇̈̉̊̋̌̍̎̏̐̑̒̓̔̽̾̿̀́͂̓̈́͆͊͋͌͐͑͒͗͛̕̚͘͜͝͠ͅ​‍‎‘’“”…€≥●☀☆☹♂✌❤➕️�🌈🏳🏻🏼👈👉👊👌👍🔥🗿😂😅😈😊😎😏😐😑😔😗😡😮😰😳🙃🙏🤑🤙🤡🤤🤭🤮🤷🥵🥶🥹🥺🦯🧑\n",
      "267\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(list(set(text)))\n",
    "numero_tokens = len(tokens)\n",
    "print(f\"VOCABULARIO: {''.join(tokens)}\")\n",
    "print(numero_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73, 80, 77, 66]\n",
      "['h', 'o', 'l', 'a']\n"
     ]
    }
   ],
   "source": [
    "char_to_int = { ch:i for i,ch in enumerate(tokens) }\n",
    "int_to_char = { i:ch for i,ch in enumerate(tokens) }\n",
    "encode = lambda str: [char_to_int[car] for car in str]\n",
    "decode = lambda lint: [int_to_char[car] for car in lint] \n",
    "\n",
    "print(encode(\"hola\"));\n",
    "print(decode(encode(\"hola\")));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([805521]) torch.int64\n",
      "tensor([19, 20, 17,  ..., 66, 84,  1])\n"
     ]
    }
   ],
   "source": [
    "import torch;\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percentage = 0.7\n",
    "train_total = int(train_percentage * len(data))\n",
    "train_set = data[:train_total]\n",
    "test_set = data[train_total:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19, 20, 17, 19, 20, 17, 20, 18, 14])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 8\n",
    "train_set[:context_length+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuando el contexto es tensor([19]) se busca el token 20\n",
      "cuando el contexto es tensor([19, 20]) se busca el token 17\n",
      "cuando el contexto es tensor([19, 20, 17]) se busca el token 19\n",
      "cuando el contexto es tensor([19, 20, 17, 19]) se busca el token 20\n",
      "cuando el contexto es tensor([19, 20, 17, 19, 20]) se busca el token 17\n",
      "cuando el contexto es tensor([19, 20, 17, 19, 20, 17]) se busca el token 20\n",
      "cuando el contexto es tensor([19, 20, 17, 19, 20, 17, 20]) se busca el token 18\n",
      "cuando el contexto es tensor([19, 20, 17, 19, 20, 17, 20, 18]) se busca el token 14\n"
     ]
    }
   ],
   "source": [
    "#Explicacion tienes dos arrays donde la letra n preferida de n-1 ... n - contexto tiene que predecir n + 1\n",
    "contexto_actual = train_set[:context_length+1]\n",
    "contexto_a_predecir = train_set[1:context_length+1]\n",
    "for letras_leidas in range(context_length):\n",
    "    contexto = contexto_actual[:letras_leidas+1]\n",
    "    a_predecir = contexto_a_predecir[letras_leidas]\n",
    "    print(f\"cuando el contexto es {contexto} se busca el token {a_predecir}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs torch.Size([4, 8])\n",
      "tensor([[84, 66, 83,  2, 69, 70,  2, 81],\n",
      "        [17, 17, 53, 74,  2, 70, 84,  2],\n",
      "        [84,  2, 78, 86, 70, 83, 85, 80],\n",
      "        [ 2, 90, 66,  2, 68, 80, 79,  2]])\n",
      "targets torch.Size([4, 8])\n",
      "tensor([[66, 83,  2, 69, 70,  2, 81, 80],\n",
      "        [17, 53, 74,  2, 70, 84,  2, 70],\n",
      "        [ 2, 78, 86, 70, 83, 85, 80, 84],\n",
      "        [90, 66,  2, 68, 80, 79,  2, 70]])\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 #batches in paralel\n",
    "context_length = 8 #context size\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_set if split == \"train\" else test_set\n",
    "    context_offset_batch = torch.randint(len(data) - context_length, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+context_length] for i in context_offset_batch])\n",
    "    y = torch.stack([data[i+1:i+context_length+1] for i in context_offset_batch])\n",
    "    return x,y\n",
    "\n",
    "tokens_in, targets = get_batch(\"train\")\n",
    "print(f\"inputs {tokens_in.shape}\\n{tokens_in}\")\n",
    "print(f\"targets {targets.shape}\\n{targets}\")\n",
    "print(\"=================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when inputed [84] target is 66\n",
      "when inputed [84, 66] target is 83\n",
      "when inputed [84, 66, 83] target is 2\n",
      "when inputed [84, 66, 83, 2] target is 69\n",
      "when inputed [84, 66, 83, 2, 69] target is 70\n",
      "when inputed [84, 66, 83, 2, 69, 70] target is 2\n",
      "when inputed [84, 66, 83, 2, 69, 70, 2] target is 81\n",
      "when inputed [84, 66, 83, 2, 69, 70, 2, 81] target is 80\n",
      "when inputed [17] target is 17\n",
      "when inputed [17, 17] target is 53\n",
      "when inputed [17, 17, 53] target is 74\n",
      "when inputed [17, 17, 53, 74] target is 2\n",
      "when inputed [17, 17, 53, 74, 2] target is 70\n",
      "when inputed [17, 17, 53, 74, 2, 70] target is 84\n",
      "when inputed [17, 17, 53, 74, 2, 70, 84] target is 2\n",
      "when inputed [17, 17, 53, 74, 2, 70, 84, 2] target is 70\n",
      "when inputed [84] target is 2\n",
      "when inputed [84, 2] target is 78\n",
      "when inputed [84, 2, 78] target is 86\n",
      "when inputed [84, 2, 78, 86] target is 70\n",
      "when inputed [84, 2, 78, 86, 70] target is 83\n",
      "when inputed [84, 2, 78, 86, 70, 83] target is 85\n",
      "when inputed [84, 2, 78, 86, 70, 83, 85] target is 80\n",
      "when inputed [84, 2, 78, 86, 70, 83, 85, 80] target is 84\n",
      "when inputed [2] target is 90\n",
      "when inputed [2, 90] target is 66\n",
      "when inputed [2, 90, 66] target is 2\n",
      "when inputed [2, 90, 66, 2] target is 68\n",
      "when inputed [2, 90, 66, 2, 68] target is 80\n",
      "when inputed [2, 90, 66, 2, 68, 80] target is 79\n",
      "when inputed [2, 90, 66, 2, 68, 80, 79] target is 2\n",
      "when inputed [2, 90, 66, 2, 68, 80, 79, 2] target is 70\n"
     ]
    }
   ],
   "source": [
    "for batch in range(batch_size):\n",
    "    for context_place in range(context_length):\n",
    "        context = tokens_in[batch, :context_place+1]\n",
    "        target_for = targets[batch, context_place]\n",
    "        print(f\"when inputed {context.tolist()} target is {target_for}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 267])\n",
      "tensor(5.9427, grad_fn=<NllLossBackward0>)\n",
      "['\\t', '͓', 'B', '̽', 'r', '●', '\\\\', 'j', '🙏', 's', '“', '?', ';', '̾', '\\xa0', '͇', 'ú', '🤷', '@', 'í', 'G', 'T', '\\\\', '#', '͓', 'u', 's', '͑', '̵', '”', '\\xa0', '´', '<', 'á', '🥺', '̯', '_', '̵', 'v', '🦯', '!', 'l', 'á', '🤤', 't', '͙', ' ', '😐', 'Ñ', '͖', '\\t', '͌', '“', 'É', '̫', '̨', 'D', '🥺', '👈', '̼', '*', '👈', '-', '🗿', '👈', '̹', 'm', '❤', '\\u200b', '➕', '̑', 'Q', '̠', '̿', '\\xa0', '͔', 'M', '͗', '̩', '͠', '\\\\', '👍', '+', '̼', '̨', ':', '͚', '~', '9', '🗿', '$', '\\U0001f979', '͓', 'ñ', 'V', '🤤', '-', '̦', '🤙', '̬', 'C']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        # batch by time by channel table (B, T, C)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            #Hay que transformarla para que funcione segun la documentacion a B*T, C\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T) #o (-1)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    #coge B T --> lo tranorma a B T+1\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx es el contexto B T\n",
    "        for _ in range(max_new_tokens):\n",
    "            #prerdicciones\n",
    "            logits, loss = self(idx)\n",
    "            #cogemos la ultima de todas\n",
    "            #porque es lo que viene ahora\n",
    "            logits = logits[:,-1,:] #B C\n",
    "            #softmax a las probabilidades\n",
    "            probs = F.softmax(logits, dim=-1) # B C\n",
    "            #cogemos un sample de la distribucion --> SOLO 1 prediccion\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)# B 1\n",
    "            #realizamos un append del sample a la secuencia B T+1\n",
    "            idx=torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(numero_tokens)\n",
    "logits, loss = m(tokens_in, targets)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(\n",
    "    decode(\n",
    "        m.generate(\n",
    "            torch.zeros((1,1), dtype=torch.long), max_new_tokens=100\n",
    "            )[0].tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "#para los gradientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2518138885498047\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    #loss evaluation\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '.', '̗', '🏻', '<', ' ', '̞', '}', '͊', '́', 'B', 'I', 'Z', '☹', '[', '5', '/', '4', '9', '.', '0', ':', ' ', '_', 'p', 'i', 'u', 'e', 'p', 'e', 'n', 'o', '.', 'a', ' ', 'p', 'i', 'u', 'n', 'g', 'e', 'a', 'l', 't', 'o', ' ', ' ', 'G', 'i', 'u', 'e', 'l', 'a', ' ', 'v', 'a', '\\n', '2', ',', ' ', 'e', ' ', 'd', '\\n', ' ', '2', ':', ' ', 'r', 'e', ' ', 'p', 'u', 's', '[', 'm', 'p', 'o', '>', '\\n', ' ', '-', ' ', '/', '5', '1', '?', '̒', '͇', 'Ñ', '´', '̛', '🥺', '̎', '̄', '̞', '😑', \"'\", '(', 'r', 'd']\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    decode(\n",
    "        m.generate(\n",
    "            torch.zeros((1,1), dtype=torch.long), max_new_tokens=100\n",
    "            )[0].tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C = 4, 8 ,32\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape\n",
    "\n",
    "x_bag_of_words = torch.zeros((B, T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] #(t, C) vector 1 dimenshon\n",
    "        x_bag_of_words[b,t] = torch.mean(xprev, 0)\n",
    "\n",
    "\n",
    "weights = torch.tril(torch.ones(T,T))\n",
    "weights = weights / torch.sum(weights, 1, keepdim=True)\n",
    "xbow2 = weights @ x # B, T, T @ B, T ,C --> B T C\n",
    "torch.allclose(xbow2, x_bag_of_words)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril==0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1);\n",
    "xbow3 = wei @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5355, 0.4645, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3592, 0.2387, 0.4021, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2119, 0.3349, 0.2025, 0.2507, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2838, 0.1680, 0.1804, 0.1936, 0.1741, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1257, 0.1426, 0.1584, 0.2308, 0.1751, 0.1674, 0.0000, 0.0000],\n",
       "         [0.1517, 0.1561, 0.1246, 0.1546, 0.1194, 0.1442, 0.1495, 0.0000],\n",
       "         [0.1593, 0.1123, 0.1371, 0.0816, 0.1285, 0.1071, 0.1458, 0.1283]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3903, 0.6097, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4576, 0.3158, 0.2266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3615, 0.2059, 0.2118, 0.2208, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2256, 0.2560, 0.1833, 0.1304, 0.2048, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1710, 0.1271, 0.1584, 0.1240, 0.2381, 0.1814, 0.0000, 0.0000],\n",
       "         [0.1632, 0.1516, 0.1178, 0.1307, 0.1547, 0.1575, 0.1246, 0.0000],\n",
       "         [0.0979, 0.0982, 0.0453, 0.1496, 0.1097, 0.1658, 0.1048, 0.2288]]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#heads of attention\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # B T 16\n",
    "q = query(x) # B T 16\n",
    "\n",
    "wei = q @ k.transpose(-2,-1) * head_size**-0.5# B T 16 @ B 16 T --> B T T \n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril==0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "wei[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jorge/Escritorio/IA/MiniGpt/train.ipynb Cell 22\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jorge/Escritorio/IA/MiniGpt/train.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jorge/Escritorio/IA/MiniGpt/train.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m checkpoint_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mstate.pth\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jorge/Escritorio/IA/MiniGpt/train.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m state \u001b[39m=\u001b[39m {\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jorge/Escritorio/IA/MiniGpt/train.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m: epoch \u001b[39m+\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jorge/Escritorio/IA/MiniGpt/train.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mstate_dict\u001b[39m\u001b[39m\"\u001b[39m: m\u001b[39m.\u001b[39mstate_dict(), \u001b[39m#modelo,\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jorge/Escritorio/IA/MiniGpt/train.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moptimizer\u001b[39m\u001b[39m\"\u001b[39m: optimizer\u001b[39m.\u001b[39mstate_dict(),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jorge/Escritorio/IA/MiniGpt/train.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlosslogger\u001b[39m\u001b[39m\"\u001b[39m: loss,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jorge/Escritorio/IA/MiniGpt/train.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jorge/Escritorio/IA/MiniGpt/train.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m torch\u001b[39m.\u001b[39msave(state, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jorge/Escritorio/IA/MiniGpt/train.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_checkpoint\u001b[39m(model, optimizer, losslogger, filename):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_name = \"state.pth\"\n",
    "state = {\n",
    "    \"epoch\": epoch +1,\n",
    "    \"state_dict\": m.state_dict(), #modelo,\n",
    "    \"optimizer\": optimizer.state_dict(),\n",
    "    \"losslogger\": loss,\n",
    "}\n",
    "torch.save(state, f\"{checkpoint_name}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, losslogger, filename):\n",
    "    starting_epoch = 0\n",
    "    if os.path.isfile(filename):\n",
    "        checkpoint = torch.load(filename)\n",
    "        starting_epoch = checkpoint[\"epoch\"]\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        optimizer.load_state_dict(optimizer[\"optimizer\"])\n",
    "        loss = checkpoint[\"losslogger\"]\n",
    "    else:\n",
    "        print(\"no checkpoint\")\n",
    "    \n",
    "    return model, optimizer, starting_epoch, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
